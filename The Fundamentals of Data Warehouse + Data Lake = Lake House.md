***The Fundamentals of Data Warehouse + Data Lake = Lake House***

# Introduction
With the evolution of Data Warehouses and Data Lakes, they have certainly become more specialized yet siloed in their respective landscapes over the last few years. Both data management technologies each have their own identities and are best used for certain tasks and needs, however they also struggle in providing some important abilities. Data Warehouse advantages are focused around analyzing structured data, OLTP, schema-on-write, SQL, and delivering ACID-compliant database transactions. Data Lake advantages are focused around analyzing all types of data (structured, semi-structured, unstructured), OLAP, schema-on-read, API connectivity, and low-cost object storage systems for data in open file formats (i.e. Apache Parquet).

Notably, Data Warehouses particularly struggle with support for advanced data engineering, data science, and machine learning. For example, their inability to store unstructured data (i.e. text, images, video, feature engineering vectors, etc.) for machine learning development. In addition, proprietary Data Warehouse software are expensive and struggle with integrating open source + cloud platform data science and data engineering tools (i.e. Python, Scala, Spark, SageMaker, Anaconda, DataRobot, SAS, R, etc.) for exploratory data analysis via notebooks, distributed compute processing, hosting deployed models, and storing inference pipeline results. System integration, data movement costs, and data staleness will even become more challenging (especially with limited technology choices at your disposal) to address in a hybrid on-premise cloud environment.

On the flip side, unfortunately, Data Lakes sometimes notoriously struggle with data quality, transactional support, data governance, and query performance issues. Data Lakes built without vital skills, key capabilities, and specialized technologies will inevitably over time turn into “Data Swamps”. This can be a tough situation to revert especially if the data volume and velocity continue to increase. Avoiding this dilemma is absolutely critical for achieving data-driven value and providing customer satisfaction to users who are dependent on having reliable fast data retrieval to perform their downstream analytics job duties for their stakeholders.

Strategically, integrating and unifying a Data Warehouse and Data Lake becomes a situation where you need the best of both worlds to flexibly and elastically build a cost-efficient resilient enterprise ecosystem that seamlessly supports business intelligence & reporting, data science, and data engineering, machine learning, and artificial intelligence, and delivery of “Big Data” 5 V’s (Volume, Variety, Velocity, Veracity, Value). This is the idea and vision behind Lake House as a new unified data architecture that stitches the best components of Data Lakes and Data Warehouses together as one.

Databricks is the industry leader and original creator of Lakehouse architecture (i.e. Delta Lake). Amazon Web Services (AWS) is another pioneer with a Lake House architecture (i.e. Lake Formation + AWS Analytics). Some of the main high level technical features and solutions of a Lake House architecture include: ACID transactions, upserts [update + insert] & deletes, schema enforcement, file compaction, batch & streaming unification, and incremental loading. The 3 main open Data Lake table formats are Delta Lake, Apache Hudi, and Apache Iceberg. All three provide similar techniques to the features mentioned. In this blog I will discuss the fundamentals, building blocks, and solutions architecture of Databricks Lakehouse and AWS Lake House.
